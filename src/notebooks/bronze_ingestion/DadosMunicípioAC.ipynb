{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814ebcf7-c4f3-4174-bb58-5c4cced59a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f56243a-5129-4d37-b0f4-5dfc5c2d45cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "import os, re, datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from io import BytesIO\n",
    "\n",
    "# Caminho base onde os XLSX estão salvos)\n",
    "LANDING_BASE = \"/Volumes/sinesp/source/landing/sinesp/municipios\"\n",
    "\n",
    "CATALOG  = \"sinesp\"\n",
    "SCHEMA   = \"bronze\"\n",
    "\n",
    "# Lista de UF exatamente como estão nas abas do Excel\n",
    "UFS = [\"AC\",\"AL\",\"AP\",\"AM\",\"BA\",\"CE\",\"DF\",\"ES\",\"GO\",\"MA\",\"MT\",\"MS\",\n",
    "       \"MG\",\"PA\",\"PB\",\"PR\",\"PE\",\"PI\",\"RJ\",\"RN\",\"RS\",\"RO\",\"RR\",\"SC\",\"SP\",\"SE\",\"TO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2438222b-0a73-4f19-90a4-fd45253b43f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Achar o arquivo XLSX mais recente no landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d420bc0-bc5a-4ef9-b2d4-361f47ed5dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# latest_path vem do passo anterior (string completa para o arquivo .xlsx no DBFS)\n",
    "# Aqui lemos o conteúdo como binário via Spark\n",
    "file_bytes = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(latest_path)        # caminho completo encontrado pelo seu código\n",
    "    .select(\"content\")\n",
    "    .head()[0]\n",
    ")\n",
    "\n",
    "# Abrimos diretamente no pandas via BytesIO\n",
    "xls = pd.ExcelFile(BytesIO(file_bytes))\n",
    "\n",
    "# Lista todas as abas\n",
    "all_sheets = xls.sheet_names\n",
    "\n",
    "# Filtra apenas as abas que correspondem às UFs desejadas\n",
    "sheet_ufs = [s for s in all_sheets if s in UFS]\n",
    "print(\"Abas detectadas (UFs):\", sheet_ufs)\n",
    "\n",
    "# Timestamp de ingestão\n",
    "ingestion_ts = datetime.datetime.utcnow()\n",
    "\n",
    "# Opcional: acumular tudo em um DataFrame único\n",
    "accum = None\n",
    "\n",
    "# Loop pelas abas filtradas\n",
    "for uf in sheet_ufs:\n",
    "    df = pd.read_excel(xls, sheet_name=uf, dtype=str)  # Lê a aba no pandas\n",
    "    # aqui segue seu tratamento...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e2d98e-bb2b-4fd9-bb52-ca6a8bb4b691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_files = (spark.read.format(\"binaryFile\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .load(LANDING_BASE)\n",
    "            .filter(F.lower(F.col(\"path\")).endswith(\".xlsx\"))\n",
    "            .select(\"path\",\"modificationTime\"))\n",
    "\n",
    "_latest = df_files.orderBy(F.col(\"modificationTime\").desc()).limit(1).collect()\n",
    "if not _latest:\n",
    "    raise FileNotFoundError(f\"Nenhum .xlsx encontrado em {LANDING_BASE}\")\n",
    "\n",
    "latest_path = _latest[0][\"path\"]\n",
    "print(f\"Usando arquivo XLSX: {latest_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901bbce5-01f0-492b-bfe0-dc027477361a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers de normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28bbce40-ba24-4967-90fb-4d67fdfc75af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def canon(name: str) -> str:\n",
    "    \"\"\"snake_case sem acentos/símbolos\"\"\"\n",
    "    x = name.strip().lower()\n",
    "    x = (x\n",
    "         .replace(\"ã\",\"a\").replace(\"á\",\"a\").replace(\"â\",\"a\").replace(\"à\",\"a\")\n",
    "         .replace(\"é\",\"e\").replace(\"ê\",\"e\")\n",
    "         .replace(\"í\",\"i\")\n",
    "         .replace(\"ó\",\"o\").replace(\"ô\",\"o\")\n",
    "         .replace(\"ú\",\"u\").replace(\"ü\",\"u\")\n",
    "         .replace(\"ç\",\"c\"))\n",
    "    x = re.sub(r\"[^a-z0-9_]+\",\"_\", x)\n",
    "    return x\n",
    "\n",
    "def normalize_columns_pdf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    mapping = {\n",
    "        \"Cód_IBGE\": \"cod_ibge\",\n",
    "        \"Municipio\": \"municipio\",\n",
    "        \"Município\": \"municipio\",\n",
    "        \"Sigla UF\": \"uf\",\n",
    "        \"Região\": \"regiao\",\n",
    "        \"Regiao\": \"regiao\",\n",
    "        \"Mês/Ano\": \"mes_ano\",\n",
    "        \"Mes/Ano\": \"mes_ano\",\n",
    "        \"Vítimas\": \"vitimas\",\n",
    "        \"Vitimas\": \"vitimas\",\n",
    "    }\n",
    "    cols = []\n",
    "    for c in pdf.columns:\n",
    "        c2 = mapping.get(c, c)\n",
    "        cols.append(c2)\n",
    "    pdf.columns = [canon(c) for c in cols]\n",
    "    return pdf\n",
    "\n",
    "# map PT/EN (os 3 primeiros chars do mês)\n",
    "MONTH_MAP = {\n",
    "    \"jan\":1,\"fev\":2,\"mar\":3,\"abr\":4,\"mai\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"set\":9,\"out\":10,\"nov\":11,\"dez\":12,\n",
    "    \"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12\n",
    "}\n",
    "\n",
    "def parse_mes_ano(s: str):\n",
    "    \"\"\"'Jan-18' → (2018, 1)   'Fev-2020' → (2020, 2)\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return (None, None)\n",
    "    ss = str(s).strip()\n",
    "    m3 = ss[:3].lower()\n",
    "    mm = MONTH_MAP.get(m3)\n",
    "    # ano: últimos 2 ou 4 dígitos\n",
    "    yy = re.findall(r\"(\\d{2,4})\", ss)\n",
    "    year = int(yy[-1]) if yy else None\n",
    "    if year and year < 100:\n",
    "        year = 2000 + year\n",
    "    return (year, mm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2244c53e-7cd7-433e-9643-7fd2c4ffee79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cria schema Bronze, se não existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3953154-df31-47dc-becc-f88aeb88f3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b025fb-3e58-42bb-80d0-28cabce4fcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Leitura por as abas com pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d285a0be-d928-4883-9e7e-1a12b329383e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_sheets = xls.sheet_names\n",
    "sheet_ufs  = [s for s in all_sheets if s in UFS]\n",
    "print(\"Abas detectadas (UFs):\", sheet_ufs)\n",
    "\n",
    "ingestion_ts = datetime.datetime.utcnow()\n",
    "accum = None  # opcional: para criar uma tabela única depois\n",
    "\n",
    "for uf in sheet_ufs:\n",
    "    print(f\"Processando UF: {uf} ...\")\n",
    "    pdf = pd.read_excel(xls, sheet_name=uf, dtype=str)  # mantém tudo como string no começo\n",
    "    if pdf.empty:\n",
    "        print(f\"  [skip] aba {uf} vazia\")\n",
    "        continue\n",
    "\n",
    "    pdf = normalize_columns_pdf(pdf)\n",
    "\n",
    "    # Mantém as colunas principais (as demais trataremos no Silver)\n",
    "    expected = [\"cod_ibge\",\"municipio\",\"uf\",\"regiao\",\"mes_ano\",\"vitimas\"]\n",
    "    existing = [c for c in expected if c in pdf.columns]\n",
    "    pdf = pdf[existing].copy()\n",
    "\n",
    "    # Força 'uf' a partir do nome da aba\n",
    "    pdf[\"uf\"] = uf\n",
    "\n",
    "    # Casts mínimos\n",
    "    if \"cod_ibge\" in pdf.columns:\n",
    "        pdf[\"cod_ibge\"] = pd.to_numeric(pdf[\"cod_ibge\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"vitimas\" in pdf.columns:\n",
    "        pdf[\"vitimas\"] = pd.to_numeric(pdf[\"vitimas\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Deriva year/month/year_month\n",
    "    if \"mes_ano\" in pdf.columns:\n",
    "        ym = pdf[\"mes_ano\"].apply(parse_mes_ano)\n",
    "        pdf[\"year\"]  = ym.apply(lambda t: t[0]).astype(\"Int64\")\n",
    "        pdf[\"month\"] = ym.apply(lambda t: t[1]).astype(\"Int64\")\n",
    "        pdf[\"year_month\"] = pd.to_datetime(\n",
    "            {\"year\": pdf[\"year\"], \"month\": pdf[\"month\"], \"day\": 1},\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    # Metadados de ingestão\n",
    "    pdf[\"_ingestion_ts\"] = ingestion_ts\n",
    "    pdf[\"_source_path\"]  = latest_path\n",
    "\n",
    "    # Para Spark\n",
    "    df = spark.createDataFrame(pdf.astype(object))\n",
    "\n",
    "    table_name = f\"{CATALOG}.{SCHEMA}.DadosMunicipio{uf}\"\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")                # primeira carga; depois troque para append + dedup\n",
    "       .option(\"overwriteSchema\",\"true\")\n",
    "       .saveAsTable(table_name))\n",
    "    print(f\"  [ok] gravado: {table_name} | linhas: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4774e4c3-52c3-423f-9949-9c6fb6c46f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"sinesp.bronze.DadosMunicipioAC\").limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DadosMunicípioAC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
