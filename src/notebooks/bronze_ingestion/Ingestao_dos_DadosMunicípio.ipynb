{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814ebcf7-c4f3-4174-bb58-5c4cced59a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68ecd47-945d-4e03-a69a-f0aa109e6490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "import os, io, csv, hashlib, datetime, re, requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "CATALOG        = \"sinesp\"\n",
    "SCHEMA_BRONZE  = \"bronze\"\n",
    "LANDING_MUNIC  = \"/Volumes/sinesp/source/landing/sinesp/municipios\"\n",
    "\n",
    "UFS = [\"AC\",\"AL\",\"AP\",\"AM\",\"BA\",\"CE\",\"DF\",\"ES\",\"GO\",\"MA\",\"MT\",\"MS\",\n",
    "       \"MG\",\"PA\",\"PB\",\"PR\",\"PE\",\"PI\",\"RJ\",\"RN\",\"RS\",\"RO\",\"RR\",\"SC\",\"SP\",\"SE\",\"TO\"]\n",
    "       \n",
    "ingestion_ts = datetime.datetime.utcnow()\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA_BRONZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c301c63b-ffc4-4e4c-b61a-b6d9a95026b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Localizar XLSX mais recente na landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d510d2d6-ee7e-4d87-9906-31fcb8010f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_files = (spark.read.format(\"binaryFile\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .load(LANDING_MUNIC)\n",
    "            .filter(F.lower(F.col(\"path\")).endswith(\".xlsx\"))\n",
    "            .select(\"path\",\"modificationTime\"))\n",
    "\n",
    "latest = df_files.orderBy(F.col(\"modificationTime\").desc()).limit(1).collect()\n",
    "if not latest:\n",
    "    raise FileNotFoundError(f\"Nenhum XLSX encontrado em {LANDING_MUNIC}\")\n",
    "\n",
    "latest_path = latest[0][\"path\"]\n",
    "print(f\"Usando arquivo de municípios: {latest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acf1eb4-429a-4cdf-826a-5668839d0c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Abrir XLSX via Spark -> Bytes -> pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a1ce1c-c042-4f2a-b08e-25c93d485933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_bytes = (spark.read.format(\"binaryFile\")\n",
    "              .load(latest_path)\n",
    "              .select(\"content\")\n",
    "              .head()[0])\n",
    "\n",
    "xls = pd.ExcelFile(BytesIO(file_bytes))\n",
    "sheet_ufs = [s for s in xls.sheet_names if s in UFS]\n",
    "print(\"Abas (UFs) detectadas:\", sheet_ufs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25c51fb-4df1-4c66-9139-de79ebcbdec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers de normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ddfa04-7421-4b67-ab71-451c1ab66366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------- Helpers que precisam existir ANTES do loop --------\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def canon(name: str) -> str:\n",
    "    \"\"\"Normaliza nomes de coluna: minúsculas, sem acentos e com _.\"\"\"\n",
    "    x = str(name).strip().lower()\n",
    "    # remoção simples de acentos (sem unidecode)\n",
    "    x = (x.replace(\"ã\",\"a\").replace(\"á\",\"a\").replace(\"â\",\"a\").replace(\"à\",\"a\")\n",
    "           .replace(\"é\",\"e\").replace(\"ê\",\"e\")\n",
    "           .replace(\"í\",\"i\")\n",
    "           .replace(\"ó\",\"o\").replace(\"ô\",\"o\")\n",
    "           .replace(\"ú\",\"u\").replace(\"ü\",\"u\")\n",
    "           .replace(\"ç\",\"c\"))\n",
    "    x = re.sub(r\"[^a-z0-9_]+\", \"_\", x)\n",
    "    x = re.sub(r\"_+\", \"_\", x).strip(\"_\")\n",
    "    return x\n",
    "\n",
    "def normalize_columns_pdf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Padroniza cabeçalhos vindos do XLSX para o layout da Bronze.\"\"\"\n",
    "    mapping = {\n",
    "        \"Cód_IBGE\": \"cod_ibge\",\n",
    "        \"Cod_IBGE\": \"cod_ibge\",\n",
    "        \"Código IBGE\": \"cod_ibge\",\n",
    "        \"Municipio\": \"municipio\",\n",
    "        \"Município\": \"municipio\",\n",
    "        \"Sigla UF\": \"uf\",\n",
    "        \"UF\": \"uf\",\n",
    "        \"Região\": \"regiao\",\n",
    "        \"Regiao\": \"regiao\",\n",
    "        \"Mês/Ano\": \"mes_ano\",\n",
    "        \"Mes/Ano\": \"mes_ano\",\n",
    "        \"Vítimas\": \"vitimas\",\n",
    "        \"Vitimas\": \"vitimas\",\n",
    "    }\n",
    "    cols = []\n",
    "    for c in pdf.columns:\n",
    "        cols.append(mapping.get(str(c), str(c)))\n",
    "    pdf.columns = [canon(c) for c in cols]\n",
    "    return pdf\n",
    "\n",
    "# Suporte para converter \"Jan-18\", \"fevereiro/2020\", \"2019-03\", etc.\n",
    "MONTH_MAP = {\n",
    "    \"jan\":1,\"fev\":2,\"mar\":3,\"abr\":4,\"mai\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"set\":9,\"out\":10,\"nov\":11,\"dez\":12,\n",
    "    # fallback em inglês se vier algo importado\n",
    "    \"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12\n",
    "}\n",
    "\n",
    "def parse_mes_ano(s: str):\n",
    "    \"\"\"Extrai (ano, mês) de strings como 'Jan-18', 'fevereiro/2020', '2021-07'.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return (None, None)\n",
    "    ss = str(s).strip()\n",
    "    # tenta AAAA-MM primeiro\n",
    "    m = re.match(r\"^\\s*(\\d{4})[-/](\\d{1,2})\\s*$\", ss)\n",
    "    if m:\n",
    "        return (int(m.group(1)), int(m.group(2)))\n",
    "    # tenta 'MMM-YY' / 'MMM/YY'\n",
    "    m3 = ss[:3].lower()\n",
    "    mm = MONTH_MAP.get(m3)\n",
    "    yy = re.findall(r\"(\\d{2,4})\", ss)\n",
    "    year = int(yy[-1]) if yy else None\n",
    "    if year and year < 100:  # 18 -> 2018\n",
    "        year = 2000 + year\n",
    "    return (year, mm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3a7fcb-a86c-4d82-ae5f-6b13183071b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Ingestão + transformação Bronze (loop por UF) ---\n",
    "accum = None\n",
    "rows_written_by_uf = []\n",
    "\n",
    "for uf in sheet_ufs:\n",
    "    print(f\"Processando aba {uf}...\")\n",
    "\n",
    "    # 1) Ler aba\n",
    "    pdf = pd.read_excel(xls, sheet_name=uf, dtype=str)\n",
    "    if pdf.empty:\n",
    "        print(f\"  [skip] aba {uf} vazia (sem linhas)\")\n",
    "        continue\n",
    "\n",
    "    # 2) Normalizar colunas\n",
    "    pdf = normalize_columns_pdf(pdf)\n",
    "\n",
    "    # 3) Selecionar colunas principais (mantém 'mes_ano' como rescue)\n",
    "    expected = [\"cod_ibge\",\"municipio\",\"uf\",\"regiao\",\"mes_ano\",\"vitimas\"]\n",
    "    existing = [c for c in expected if c in pdf.columns]\n",
    "    pdf = pdf[existing].copy()\n",
    "\n",
    "    # 4) UF da aba\n",
    "    pdf[\"uf\"] = uf\n",
    "\n",
    "    # 5) Casts mínimos (aceitando nulos)\n",
    "    if \"cod_ibge\" in pdf.columns:\n",
    "        pdf[\"cod_ibge\"] = pd.to_numeric(pdf[\"cod_ibge\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"vitimas\" in pdf.columns:\n",
    "        pdf[\"vitimas\"] = pd.to_numeric(pdf[\"vitimas\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # 6) Derivar year, month, year_month (parser robusto)\n",
    "    if \"mes_ano\" in pdf.columns:\n",
    "        ym = pdf[\"mes_ano\"].apply(parse_mes_ano)\n",
    "        pdf[\"year\"]  = ym.apply(lambda t: t[0] if pd.notna(t[0]) else pd.NA).astype(\"Int64\")\n",
    "        pdf[\"month\"] = ym.apply(lambda t: t[1] if pd.notna(t[1]) else pd.NA).astype(\"Int64\")\n",
    "        ys = pdf[\"year\"].astype(\"string\")\n",
    "        ms = pdf[\"month\"].astype(\"string\").str.zfill(2)\n",
    "        pdf[\"year_month\"] = pd.to_datetime(ys + \"-\" + ms + \"-01\", errors=\"coerce\")\n",
    "    else:\n",
    "        pdf[\"year\"]       = pd.Series(dtype=\"Int64\")\n",
    "        pdf[\"month\"]      = pd.Series(dtype=\"Int64\")\n",
    "        pdf[\"year_month\"] = pd.to_datetime(pd.Series(dtype=\"string\"), errors=\"coerce\")\n",
    "\n",
    "    # 7) Metadados\n",
    "    pdf[\"_ingestion_ts\"] = ingestion_ts\n",
    "    pdf[\"_source_path\"]  = latest_path\n",
    "\n",
    "    # 8) Checks leves (não derruba por year_month nulo na Bronze)\n",
    "    before = len(pdf)\n",
    "    pdf = pdf[pdf[\"uf\"] == uf]  # consistência de UF\n",
    "    if \"vitimas\" in pdf.columns:\n",
    "        pdf = pdf[pdf[\"vitimas\"].isna() | (pdf[\"vitimas\"] >= 0)]  # sem negativos\n",
    "    after = len(pdf)\n",
    "    if after < before:\n",
    "        print(f\"  [warn] {before - after} linha(s) removida(s) por checks leves\")\n",
    "\n",
    "    # Se ficou vazio, pula\n",
    "    if pdf.empty:\n",
    "        print(f\"  [skip] aba {uf} ficou vazia após checks leves\")\n",
    "        continue\n",
    "\n",
    "    # 9) Para Spark + ajuste de tipos finais\n",
    "    df = (spark.createDataFrame(pdf.astype(object))\n",
    "          .withColumn(\"year\",       F.col(\"year\").cast(T.IntegerType()))\n",
    "          .withColumn(\"month\",      F.col(\"month\").cast(T.IntegerType()))\n",
    "          .withColumn(\"year_month\", F.to_date(\"year_month\"))\n",
    "          .withColumn(\"cod_ibge\",   F.col(\"cod_ibge\").cast(T.LongType()))\n",
    "          .withColumn(\"vitimas\",    F.col(\"vitimas\").cast(T.IntegerType()))\n",
    "          .withColumn(\"_ingestion_ts\", F.col(\"_ingestion_ts\").cast(T.TimestampType()))\n",
    "         )\n",
    "\n",
    "    # 10) Escrever tabela por UF (1ª vez overwrite; depois use 'append')\n",
    "    tbl = f\"{CATALOG}.{SCHEMA_BRONZE}.DadosMunicipio{uf}\"\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"overwriteSchema\",\"true\")\n",
    "       .saveAsTable(tbl))\n",
    "    n = df.count()\n",
    "    rows_written_by_uf.append((uf, n))\n",
    "    print(f\"  [ok] gravado: {tbl} | linhas: {n}\")\n",
    "\n",
    "    # 11) Acumular para unificada\n",
    "    accum = df if accum is None else accum.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# --- Tabela unificada (fora do loop) ---\n",
    "if accum is not None:\n",
    "    unified_tbl = f\"{CATALOG}.{SCHEMA_BRONZE}.municipios_raw_row\"\n",
    "    (accum.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")              # nas próximas execuções, trocar para 'append'\n",
    "       .option(\"overwriteSchema\",\"true\")\n",
    "       .partitionBy(\"uf\",\"year\",\"month\")\n",
    "       .saveAsTable(unified_tbl))\n",
    "    print(f\"[ok] gravado também: {unified_tbl} (particionado por uf/year/month)\")\n",
    "else:\n",
    "    print(\"Nenhuma aba processada.\")\n",
    "\n",
    "print(\"Resumo por UF (linhas escritas):\", rows_written_by_uf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestao_dos_DadosMunicípio",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
