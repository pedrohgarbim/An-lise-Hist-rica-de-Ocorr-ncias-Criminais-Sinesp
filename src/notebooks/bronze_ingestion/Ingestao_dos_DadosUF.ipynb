{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc9c747-2504-4c4c-a85b-719a8557d3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parametros Iniciais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3b8d84-34a3-411c-a374-6f14eabf7535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "import os, io, csv, hashlib, datetime, re, requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "CATALOG        = \"sinesp\"\n",
    "SCHEMA_BRONZE  = \"bronze\"\n",
    "LANDING_UF  = \"/Volumes/sinesp/source/landing/sinesp/uf\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA_BRONZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a31a7f7-7067-4c7e-a3a9-dda7365fea28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Localizar XLSX mais recente na landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1915b5-78bd-4996-a773-8cd027afe229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_files = (spark.read.format(\"binaryFile\")\n",
    "            .option(\"recursiveFileLookup\",\"true\")\n",
    "            .load(LANDING_UF)\n",
    "            .filter(F.lower(F.col(\"path\")).endswith(\".xlsx\"))\n",
    "            .select(\"path\",\"modificationTime\"))\n",
    "\n",
    "latest = df_files.orderBy(F.col(\"modificationTime\").desc()).limit(1).collect()\n",
    "if not latest:\n",
    "    raise FileNotFoundError(f\"Nenhum XLSX encontrado em {LANDING_UF}\")\n",
    "\n",
    "latest_path = latest[0][\"path\"]\n",
    "print(\"Usando arquivo UF:\", latest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c1107d-20a1-46b0-9ff1-87b60a097d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Abrir XLSX via Spark -> Bytes -> pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0081c971-fec0-477c-9ba0-c21f1d28e0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_bytes = (spark.read.format(\"binaryFile\")\n",
    "              .load(latest_path)\n",
    "              .select(\"content\")\n",
    "              .head()[0])\n",
    "\n",
    "xls = pd.ExcelFile(BytesIO(file_bytes))\n",
    "print(\"Abas encontradas:\", xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c7425a-19d4-4d3d-8c05-324adefd3906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers de normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36207e65-82ca-4062-9066-758e52634f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def canon(s: str) -> str:\n",
    "    x = s.strip().lower()\n",
    "    x = (x.replace(\"ã\",\"a\").replace(\"á\",\"a\").replace(\"â\",\"a\").replace(\"à\",\"a\")\n",
    "           .replace(\"é\",\"e\").replace(\"ê\",\"e\")\n",
    "           .replace(\"í\",\"i\")\n",
    "           .replace(\"ó\",\"o\").replace(\"ô\",\"o\")\n",
    "           .replace(\"ú\",\"u\").replace(\"ü\",\"u\")\n",
    "           .replace(\"ç\",\"c\"))\n",
    "    return re.sub(r\"[^a-z0-9_]+\",\"_\", x)\n",
    "\n",
    "def find_sheet(xls: pd.ExcelFile, wanted: str):\n",
    "    w = canon(wanted)\n",
    "    for s in xls.sheet_names:\n",
    "        if canon(s) == w:\n",
    "            return s\n",
    "    # tentativas mais soltas (ocorrencias/vitimas sem acento)\n",
    "    for s in xls.sheet_names:\n",
    "        cs = canon(s)\n",
    "        if w in cs:\n",
    "            return s\n",
    "    return None\n",
    "\n",
    "MONTH_MAP = {\n",
    "    \"janeiro\":1,\"fevereiro\":2,\"marco\":3,\"março\":3,\"abril\":4,\"maio\":5,\"junho\":6,\n",
    "    \"julho\":7,\"agosto\":8,\"setembro\":9,\"outubro\":10,\"novembro\":11,\"dezembro\":12,\n",
    "    \"jan\":1,\"fev\":2,\"mar\":3,\"abr\":4,\"mai\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"set\":9,\"out\":10,\"nov\":11,\"dez\":12,\n",
    "    \"jan.\":1,\"fev.\":2,\"mar.\":3,\"abr.\":4,\"mai.\":5,\"jun.\":6,\"jul.\":7,\"ago.\":8,\"set.\":9,\"out.\":10,\"nov.\":11,\"dez.\":12\n",
    "}\n",
    "\n",
    "def month_from_text(x):\n",
    "    if pd.isna(x): return pd.NA\n",
    "    s = str(x).strip().lower()\n",
    "    # se vier como data\n",
    "    try:\n",
    "        ts = pd.to_datetime(x, errors=\"coerce\")\n",
    "        if pd.notna(ts):\n",
    "            return int(ts.month)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # busca no dicionário\n",
    "    return MONTH_MAP.get(s, pd.NA)\n",
    "\n",
    "UF_SIGLA = {\n",
    "    \"acre\":\"AC\",\"alagoas\":\"AL\",\"amapa\":\"AP\",\"amapá\":\"AP\",\"amazonas\":\"AM\",\"bahia\":\"BA\",\"ceara\":\"CE\",\"ceará\":\"CE\",\n",
    "    \"distrito federal\":\"DF\",\"espirito santo\":\"ES\",\"espírito santo\":\"ES\",\"goias\":\"GO\",\"goiás\":\"GO\",\"maranhao\":\"MA\",\"maranhão\":\"MA\",\n",
    "    \"mato grosso\":\"MT\",\"mato grosso do sul\":\"MS\",\"minas gerais\":\"MG\",\"para\":\"PA\",\"pará\":\"PA\",\"paraiba\":\"PB\",\"paraíba\":\"PB\",\n",
    "    \"parana\":\"PR\",\"paraná\":\"PR\",\"pernambuco\":\"PE\",\"piaui\":\"PI\",\"piauí\":\"PI\",\"rio de janeiro\":\"RJ\",\"rio grande do norte\":\"RN\",\n",
    "    \"rio grande do sul\":\"RS\",\"rondonia\":\"RO\",\"rondônia\":\"RO\",\"roraima\":\"RR\",\"santa catarina\":\"SC\",\"sao paulo\":\"SP\",\"são paulo\":\"SP\",\n",
    "    \"sergipe\":\"SE\",\"tocantins\":\"TO\"\n",
    "}\n",
    "\n",
    "def uf_to_sigla(x):\n",
    "    if pd.isna(x): return pd.NA\n",
    "    s = str(x).strip().lower()\n",
    "    return UF_SIGLA.get(s, s.upper() if len(s)==2 else pd.NA)\n",
    "\n",
    "ingestion_ts = datetime.datetime.utcnow()\n",
    "\n",
    "# ---------- 4) Ocorrências ----------\n",
    "sheet_occ = find_sheet(xls, \"Ocorrências\")\n",
    "if not sheet_occ:\n",
    "    raise ValueError(\"Aba 'Ocorrências' não encontrada no arquivo UF.\")\n",
    "\n",
    "pdf_occ = pd.read_excel(xls, sheet_name=sheet_occ, dtype=str)\n",
    "# mapeia nomes de colunas\n",
    "map_occ = {\n",
    "    \"UF\":\"uf_nome\",\n",
    "    \"Tipo Crime\":\"tipo_crime\",\n",
    "    \"Ano\":\"year\",\n",
    "    \"Mês\":\"month_text\",\n",
    "    \"Ocorrências\":\"ocorrencias\"\n",
    "}\n",
    "cols = {c: map_occ.get(c, c) for c in pdf_occ.columns}\n",
    "pdf_occ.rename(columns=cols, inplace=True)\n",
    "# normaliza\n",
    "pdf_occ[\"uf_nome\"]    = pdf_occ[\"uf_nome\"].astype(str).str.strip()\n",
    "pdf_occ[\"uf_sigla\"]   = pdf_occ[\"uf_nome\"].apply(uf_to_sigla)\n",
    "pdf_occ[\"tipo_crime\"] = pdf_occ[\"tipo_crime\"].astype(str).str.strip()\n",
    "\n",
    "pdf_occ[\"year\"]  = pd.to_numeric(pdf_occ[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "pdf_occ[\"month\"] = pdf_occ[\"month_text\"].apply(month_from_text).astype(\"Int64\")\n",
    "pdf_occ.drop(columns=[\"month_text\"], inplace=True)\n",
    "\n",
    "pdf_occ[\"ocorrencias\"] = pd.to_numeric(pdf_occ[\"ocorrencias\"], errors=\"coerce\").astype(\"Int64\")\n",
    "pdf_occ[\"year_month\"]  = pd.to_datetime(\n",
    "    pdf_occ[\"year\"].astype(\"string\") + \"-\" + pdf_occ[\"month\"].astype(\"string\").str.zfill(2) + \"-01\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "pdf_occ[\"_ingestion_ts\"] = ingestion_ts\n",
    "pdf_occ[\"_source_path\"]  = latest_path\n",
    "\n",
    "# checks leves (não negativos, UF reconhecida)\n",
    "before = len(pdf_occ)\n",
    "pdf_occ = pdf_occ[(pdf_occ[\"uf_sigla\"].notna()) & (pdf_occ[\"ocorrencias\"].isna() | (pdf_occ[\"ocorrencias\"] >= 0))]\n",
    "after  = len(pdf_occ)\n",
    "print(f\"[Ocorrências] linhas removidas por checks leves: {before - after}\")\n",
    "\n",
    "# envia ao Spark e escreve\n",
    "df_occ = (spark.createDataFrame(pdf_occ.astype(object))\n",
    "          .withColumn(\"year\",       F.col(\"year\").cast(T.IntegerType()))\n",
    "          .withColumn(\"month\",      F.col(\"month\").cast(T.IntegerType()))\n",
    "          .withColumn(\"year_month\", F.to_date(\"year_month\"))\n",
    "          .withColumn(\"ocorrencias\",F.col(\"ocorrencias\").cast(T.IntegerType()))\n",
    "          .withColumn(\"_ingestion_ts\", F.col(\"_ingestion_ts\").cast(T.TimestampType()))\n",
    "         )\n",
    "\n",
    "tbl_occ = f\"{CATALOG}.{SCHEMA_BRONZE}.ufocorrencias\"\n",
    "(df_occ.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")     # após a 1ª carga, use 'append'\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .partitionBy(\"uf_sigla\",\"year\",\"month\")\n",
    "   .saveAsTable(tbl_occ))\n",
    "print(f\"[ok] gravado: {tbl_occ} | linhas: {df_occ.count()}\")\n",
    "\n",
    "# ---------- 5) Vítimas ----------\n",
    "sheet_vit = find_sheet(xls, \"Vítimas\")\n",
    "if not sheet_vit:\n",
    "    raise ValueError(\"Aba 'Vítimas' não encontrada no arquivo UF.\")\n",
    "\n",
    "pdf_vit = pd.read_excel(xls, sheet_name=sheet_vit, dtype=str)\n",
    "map_vit = {\n",
    "    \"UF\":\"uf_nome\",\n",
    "    \"Tipo Crime\":\"tipo_crime\",\n",
    "    \"Ano\":\"year\",\n",
    "    \"Mês\":\"month_text\",\n",
    "    \"Sexo da Vítima\":\"sexo_vitima\",\n",
    "    \"Vítimas\":\"vitimas\"\n",
    "}\n",
    "cols = {c: map_vit.get(c, c) for c in pdf_vit.columns}\n",
    "pdf_vit.rename(columns=cols, inplace=True)\n",
    "\n",
    "pdf_vit[\"uf_nome\"]    = pdf_vit[\"uf_nome\"].astype(str).str.strip()\n",
    "pdf_vit[\"uf_sigla\"]   = pdf_vit[\"uf_nome\"].apply(uf_to_sigla)\n",
    "pdf_vit[\"tipo_crime\"] = pdf_vit[\"tipo_crime\"].astype(str).str.strip()\n",
    "pdf_vit[\"sexo_vitima\"]= pdf_vit[\"sexo_vitima\"].astype(str).str.strip()\n",
    "\n",
    "pdf_vit[\"year\"]  = pd.to_numeric(pdf_vit[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "pdf_vit[\"month\"] = pdf_vit[\"month_text\"].apply(month_from_text).astype(\"Int64\")\n",
    "pdf_vit.drop(columns=[\"month_text\"], inplace=True)\n",
    "\n",
    "pdf_vit[\"vitimas\"]    = pd.to_numeric(pdf_vit[\"vitimas\"], errors=\"coerce\").astype(\"Int64\")\n",
    "pdf_vit[\"year_month\"] = pd.to_datetime(\n",
    "    pdf_vit[\"year\"].astype(\"string\") + \"-\" + pdf_vit[\"month\"].astype(\"string\").str.zfill(2) + \"-01\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "pdf_vit[\"_ingestion_ts\"] = ingestion_ts\n",
    "pdf_vit[\"_source_path\"]  = latest_path\n",
    "\n",
    "# checks leves\n",
    "before = len(pdf_vit)\n",
    "pdf_vit = pdf_vit[(pdf_vit[\"uf_sigla\"].notna()) & (pdf_vit[\"vitimas\"].isna() | (pdf_vit[\"vitimas\"] >= 0))]\n",
    "after  = len(pdf_vit)\n",
    "print(f\"[Vítimas] linhas removidas por checks leves: {before - after}\")\n",
    "\n",
    "df_vit = (spark.createDataFrame(pdf_vit.astype(object))\n",
    "          .withColumn(\"year\",       F.col(\"year\").cast(T.IntegerType()))\n",
    "          .withColumn(\"month\",      F.col(\"month\").cast(T.IntegerType()))\n",
    "          .withColumn(\"year_month\", F.to_date(\"year_month\"))\n",
    "          .withColumn(\"vitimas\",    F.col(\"vitimas\").cast(T.IntegerType()))\n",
    "          .withColumn(\"_ingestion_ts\", F.col(\"_ingestion_ts\").cast(T.TimestampType()))\n",
    "         )\n",
    "\n",
    "tbl_vit = f\"{CATALOG}.{SCHEMA_BRONZE}.ufvitimas\"\n",
    "(df_vit.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")     # após a 1ª carga, use 'append'\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .partitionBy(\"uf_sigla\",\"year\",\"month\")\n",
    "   .saveAsTable(tbl_vit))\n",
    "print(f\"[ok] gravado: {tbl_vit} | linhas: {df_vit.count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestao_dos_DadosUF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
